---
layout: post
title:  "Bounded Adversarial Divergence"
date:   2022-09-12 10:10:00 +0100
categories: ai;code;experiments;articles
img: badneurips.png
preview: "Submitted at NeurIPS Creativity Workshop 2022"
---
<img src="https://github.com/domkirke/divergent-synthesis/blob/main/assets/extrap_v2.png?raw=true" style="width: 100%; margin-bottom: 15px"/>
<div style="font-size: 1.7vw; font-weight: 150; margin-bottom: 15px;">
Machine learning approaches now achieve impressive generation capabilities in numerous domains such as image, audio or video. However, most training & evaluation frameworks revolve around the idea of strictly modelling the original data distribution rather than trying to extrapolate from it. This precludes the ability of such models to diverge from the original distribution and, hence, exhibit some creative traits. In this paper, we propose various perspectives on how this complicated goal could ever be achieved, and provide preliminary results on our novel training objective called <emph style="font-weight: 600; display: inline;">Bounded Adversarial Divergence</emph> (BAD).
</div>

<div style="margin-bottom: 15px;">
  <div style="display: flex; align-items: stretch; justify-content: space-around; font-size: 1.9vw; font-weight: 150">
    <div style="display: inline-block;"><a href="https://github.com/domkirke/divergent-synthesis">Github</a></div>
    <div style="display: inline-block;"><a href="https://arxiv.org/abs/2211.08861">Paper</a></div>
  </div>
</div>