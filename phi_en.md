---
layout: default
---


# Documentation of earlier projects

## AI-related musical works

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/87S43pbpMY4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</center>
<br/>
<em>aletheia</em> (2022) is a musical piece based on exploring, transforming, hacking, bending, and interplaying with audio generative models. Referring to the pre-socratic notion of aletheia, this piece illustrates the discovery of reality of neural audio synthesis models by going from a pure phenomenological perception to an imitation game, yielding to a deranging and coercive version of reality. Besides attempting a reflexive work between philosophical aspects of reality and exploration of model’s rules through deviation, aletheia
is motivated by the experimental development of new approaches to compose and interact with neural generative models, and by which new aesthetics they can provide. It has been played in Cirque Électrique (Paris), La Haye (Netherlands), and Tokyo (Japan).

<br/>

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/zGnvID6EMbU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</center>
<em>aego</em> (2018; 2022)  is a performance that stages a person and a machine that learn to improvise in music. The machine uncertainly explores a spectral latent space, while the person communicates positive or ne- gative reinforcement, turning their hands front or back, in an attempt to teach the machine how to explore this space. Gradually, the person gives up communicating consistent reinforcement to the machine, leaving the machine’s learning undetermined. Freed from the obligation to record their preferences into the machine, the person engages in deep listening through sound, and learns to improvise in music.

<br/>
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/f9-iXlb-ADw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</center>
<em>you will get what you leave behind</em> (2023) is a mixed performance based on real-time exploration of neural audio synthesis devices. Structured as a tryptych, this performance exhibits the complex relationship between these modern technologies and the materiality of their memory, unveiling simultaneously their underlying hanthology (in anguish but also in onirism) and their materiality (by their nature jointly real and unreal, living and unliving), thus offering novel monsters, sometimes familiar, sometimes uncanny, if not terrifying. The project is now resident in Château Éphèmère (France), and will be produced in the course of 2024.

<br/>
<center>
<iframe style="border: 0; width: 350px; height: 470px;" src="https://bandcamp.com/EmbeddedPlayer/album=3240089844/size=large/bgcol=ffffff/linkcol=0687f5/tracklist=false/transparent=true/" seamless><a href="https://daim.bandcamp.com/album/natures-d-couverte">Natures&amp;Découverte de Daim™</a></iframe>
<iframe style="border: 0; width: 350px; height: 470px;" src="https://bandcamp.com/EmbeddedPlayer/album=2012367212/size=large/bgcol=ffffff/linkcol=0687f5/tracklist=false/transparent=true/" seamless><a href="https://daim.bandcamp.com/album/natures-d-couverte-the-tenth-tenth-anniversary-edition">Natures&amp;D​é​couverte The Tenth Tenth Anniversary Edition de Daim™</a></iframe>
</center>
<em>Nature&Découvertes</em> by the trio Daim™ explores the codes advertisement music by musicking the 24 hours of a sleepless mid-class worker covered by a mythical creative insurance called w.lfg.ng. A «1010th anniversary edition», released in 2023, is a side-album where every track are fed to a ge- nerative AI, trained on the Daim™ discography, in an increasing manner of times. This process then yields to progressive vanishing and oblivion, due to the over-normalization of the process performed by the the AI.
<br/>
<br/>
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/8nYz_kTjetA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</center>
<em>deepscape</em> (2023) is a collaboration with Hugo Scurto. deepscape is a conceptual and technical work toward practicing and theorising through/out the deepscape, which may designate the global flows of media intensively computed by deep learning throughout the Internet, entangled with the material, human and cultural resources they capitalise on throughout corporate infrastructures of artificial intelligence (AI). This project aims to explore deep listening of soundscapes generated by deep learning as a practice to raise awareness of the planetary scale of the deepscape, by training a [RAVE](https://github.com/acids-ircam/RAVE) model on worldwide soundscape data that I transversally recorded over 28 places in late April 2022, using the Locustream online sound map.
<br/>
<br/>
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/videoseries?list=PLScv8n5132x_6OR7cPQ2yWW4GDtOboRWW" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</center>
<em>ACIDS workshop</em> (2023-) is a series of live shows I initiated at IRCAM with researchers, composers & musicians. The purpose, with this free-entry show, was to offer a carte blanche to anyone interested in music making with AI-powered neural synthesis, without any aesthetic exigence but the live encounter between a research
team, performers, composers, and their audience. The first edition, that took place in January 2022, was then followed by an edition with circus artists of the CNAC (Centre National des Arts du Cirques), and followed by a second edition in November 2022.


## AI-related artistical works

{% include image-gallery.html folder="/assets/images/works/inception-ghosts-i" %}
<em>Inception Ghosts</em> is a series of AI-generated pictures, that intends to exhibit the veiled criteria of machine-learning based classification algorithms for images (here, Inception). An randomly initialized generator is trained to maximize the objective that classifiers have to maximize to judge the output's quality ; this way, the generator can somehow «absorbe» their internal working, providing abstract images that, paradoxically, are optimally classified.

{% include image-gallery.html folder="/assets/images/works/cloud" %}
[The Cloud](https://arkadizaides.com/the-cloud) is a perfromance by Arkadi Zaides that puts the Chernobyl catastrophe into the limelight by following the movement of the radioactive cloud, its fallout, and the danger it still represents for humans. This project leverages several AI-based text, image, and audio generative algorithms of explore on-stage the very limits of reality and belief. This project is produced by Instituts des Croisements and co-produced by Montpellier Danse (FR), laGeste (BE), Charleroi Danse (BE), Maison de la Danse (FR) Residency support PACT Zollverein (DE), Teatro Biblioteca Quarticciolo (IT), CAMPO (BE), and with the support of The Ministry of Culture of France / Directorate General for Artistic Creation, TMU New York


## AI-related works

### Code
[vschaos](https://github.com/acids-ircam/vschaos2) is a spectral auto-encoders that can be used for long-evolving spectro-morphologies, that can performed in real time using the Max software. This work, that was one of my PhD products and was then one of the first usable real-time AI generative model, was the one used in the aego performance in 2019. I recently re-coded it to be available with modern architectures, providing a lightweight of performing explorative neural audio synthesis.

[nn~](https://github.com/acids-ircam/nn_tilde) initiated by Antoine Caillon is worldwide-used general framework to embed neural audio synthesis in real-time uses within user-compliant software such as PureData and Max for real-time AI performance. I took part in the development after the first release, adding multiple features and still working on creative network-bending possibilities and multi-modality.

[RAVE-VST](https://github.com/acids-ircam/rave_vst) is a VST embedding RAVE as a DAW-embeddable synthesizer, whose I developed signal processing and contributed to UI design.

[divergent-synthesis](https://github.com/domkirke/divergent-synthesis) are first experiments of an extrapolative learning framework for generative IAs called (Bounded Adversarial Divergence), based on the divergent fine-tuning of pre-trained models using neural classifiers. 

<em>active_divergence</em> is a toolbox for applying various alteration methods to audio, visual and textual generative models, in command or in real-time: adaptation, network bending, rewriting. To be published in the coming months.

### Slideshows
[Presentation in Sorbonne Universités](/assets/documents/sorbonne.pdf) for Textures Électroniques seminar, organized by artist Kaspar Ravel (2023)


### Article selection
- Scurto H. & Chemla—Romeu-Santos Axel, [Deeply Listening Through/Out the Deepscape](https://hal.science/hal-04108995/file/scurto2023deeply.pdf), ISEA2023
- Chemla–Romeu-Santos A. & Esling P., [Creative divergent synthesis with generative models](https://domkirke.github.io/ai;code;experiments;articles/2022/09/12/badneurips.html) (arXiv)
- Chemla–Romeu-Santos A., [aletheia](/assets/documents/aletheia.pdf), AIMC2022 (musical paper)
- Chemla–Romeu-Santos A. & Esling P., [Challenges in creative generative models: a divergence maximization perspective](https://arxiv.org/pdf/2211.08856.pdf), AIMC2022 (research paper)
- Chemla–Romeu-Santos A., [Représentations variationnelles inversibles: Une nouvelle approche pour la synthèse sonore](https://hal.archives-ouvertes.fr/hal-03353913/), JIM, 2020
- Chemla–Romeu-Santos A., Scurto H. [Machine Learning for Computer Music Multidisciplinary Research: A Practical Case Study](https://hal.archives-ouvertes.fr/hal-02408699/). CMMR, 2019. 
- Chemla–Romeu-Santos A., Ntalampiras S., Esling P., Haus G., Assayag G. [Cross-Modal Variational Inference for Bijective Signal-Symbol Translation](https://hal.archives-ouvertes.fr/hal-02471810/). DAFX Proceedings 2019 (2019). 
- Esling, P., Chemla—Romeu-Santos, A., Bitton, A. [Bridging audio analysis, perception and synthesis with perceptually-regularized variational timbre spaces](http://ismir2018.ircam.fr/doc/pdfs/219_Paper.pdf) , ISMIR2018 (2018) 
- Nika, J., Déguernel, K., Chemla—Romeu-Santos, A. [DYCI2 agents: merging the" free"," reactive", and" scenario-based" music generation paradigms](https://hal.science/hal-01583089/file/DYCI2_CreativeAgents_Nika_al.pdf). International Computer Music Conference (2017)